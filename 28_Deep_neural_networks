Deep neural networks in genomics - 22nd October 2019

Deep learning in genomics - Zou et al Nature Genetics 2019 (base calling etc)

Neural networks - probability modelled by sigmoidal curve (0 for false, 1 for true)
Tweak parameters to get best predictions (Input data - observations as training set)
Create different networks/pathways to combine parameters to get single output (non-linear) - prediction

Non-linear models - more power - Gokcen Nature Reviews Genetics 2019 (https://www.nature.com/articles/s41576-019-0122-6)
(lots of figures from presentation are from this paper!)
Single-layer neural network (logistic regression) v multilayer neural network
Non-linear activation - taking something linear and making it non-linear (remove values below 0?)
Optimise parameters; minimise loss function l by calculating gradient (dl/d0 - differentiation) in 2D

Training process:
  start with random parameters
  take batch of inputs
  calculate predictions
  calculate performance (loss)
  calculate gradient for every parameter
  adjust parameters along gradient
go back to 2. until no noticeable improvement in performance

Training set, validation set and test set
Validation set used to monitor for performance loss eg overfitting (don't want to memorise training set, want to be more generalised)

Network types (and input examples)
  fully connected (number of k-mer matches)
  convolutional (DNA, a.a sequence, image)
  recurrent (DNA, aa.sequence, time)
  graph convolutional (protein-protein network, protein structure)
  
Convolutional networks for image analysis
  Convert 4x4 grid of greyscale values into single value by applying filter (can parameterise)
  Use for edge detection (use Sobel operator as filter)
  More complexity when have deeper layers
  
Convolutional networks for DNA
  apply filter to DNA sequence - hot coded sequence - transform DNA sequence into binary sequence 
  (table of 4 rows by x columns where x = length of sequence)
  score - how well does motif match particular sequence? Remove all negative values before continuing to next layer
  
Convolutional neural networks in genomics
  chromatin feature networks - transcription factor binding sites, open chromatin, histone marks
  apply ~300 filters for first layer - work out number of filters and actual values by iteration. Multiple layers
  scan genome every ~1kb
  example - Angermueller et al. 2016
  need minimum 10,000s data points for deep learning to avoid overfitting, otherwise just use random forests

Interpretation - less straight forward!
  feature importance scores (like random forest)
  pertubation based (compute perturbation compared to reference sequence for each base)-> importance scores and pertubation impact
  backpropagation based - similar to above but will get score for chunk of sequence rather than for each base
  
Learning strategy
  single task (one output - info on one transcription factor) 
  multitask (two outputs, info on two transcription factors)
  multimodal (two inputs, DNA and info on chromatin accessibility) 
  transfer learning (two inputs, DNA sequence and three outputs - three transcription factors. Use pre-existing model made with 
  training set to find different transcription factor)
  
Practical aspects
  Under-fitting (too simple to explain variance) v over-fitting (force fitting - too good to be true)
  Under-fitting leads to high bias and low variance - predictions will be too close to each other
  Over-fitting leads to high variance and low bias
  Train, validation and test sets (plus cross-validation if low on data points) - usually 10% of total data set (or completely new set)
  Stratify sets so all variables representative across all three sets
  Cross-validation - use same data set but change which bit is training and which is test each time - separate model each time,
  take average final measure of performance (rather than inputting same data into same model multiple times)
  
Prevent overfitting - optimise regularisation (neuronal/node dropout - force model to learn redundant pathways), use more data, 
early stopping (training rounds/number of iterations), ensemble learning
Prevent underfitting - increase complexity of model (more filters, make filters bigger, more layers), use more data

Link to tutorial (from github Colab notebook deep learning genomics intro - free Jupyter notebook that runs on cloud,
https://colab.research.google.com/drive/1SRHe_SXmKeXImNBR6tnhFQ3eThM4-iZu#scrollTo=59DLQSAO7PBm)
