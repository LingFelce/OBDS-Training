#Dimensionality reduction and clustering (statistics) 8th October 2019

#Dimensionality reduction
Many genes not interesting as don't vary a lot, or don't have enough counts - filtering needed
Use dimensionality reduction methods to find structure in data, aid visualisation
Unsupervised learning for finding groups of homogenous items eg PCA or t-SNE

#Principal component analysis
Find linear combo of variables to create principal components, maintain most variance in data
Principal components uncorrelated (orthogonal to each other) and ordered with respect to % variance
Assumes relationship between variables is linear
Spectral decomposition, singular value decomposition
Centre data first (mean centre), scale data if using different units (use correlation matrix instead of covariance matrix)
Visualise top genes associated with principal components (PC1 or PC2) - top/bottom loadings - proportion of variance in gene can be
explained by PC1 or PC2.
Can combine PCA analysis with log expression (overlay) for specific gene

#t-SNE: t-Distributed Stochastic Neighbour Embedding
visualisation of high dimensional datasets
Non-linear dimensionality reduction, conserves structure of data
Places cells with similar local neighbourhoods in high dimensional space together in low dimensional space
R implementation - https://lvdmaaten.github.io/tsne
Measure distances between all points in high dimensional space. Use normal density to measure similarity between points.
Distant points will have low similarity values, nearby points will have large similarity values.
Scale similarities to account for density of data. Do same for points in lower dimensional space, but with t-distribution.

#Clustering
Grouping given data points and classification into specific groups. Method of unsupervised learning.
K-means clustering - select number of classes/groups (k)
Initialise centre points. Distance between each point and group centre is computed. Random initialisation, may be different each time??
May work better with larger datasets (more samples)
Hierarchical clustering - use a metric (distance between observations) eg Euclidean distance.
Either a bottom up or top down approach. (more commonly used in heatmaps etc - cluster dendrogram)

#Notes
ggplot doesn't like loops in R, have to add extra things. 
ggplot likes to have data tables in wide format, so have to use melt function.

```{r get data}
raw_counts <- read.table("GTEx_subset_1000_subset.txt", header=T, sep="")

info <- read.table("GTEx_subset_1000_subset_info.txt", header=T, sep="\t")

```

Data filtering
```{r filter}

#distribution of read counts 
hist(as.numeric(rowSums(raw_counts)),
     breaks = 100, main = "Expression sum per gene",
     xlab = "Sum expression")
abline(v=median(as.numeric(rowSums(raw_counts))),col="red")

#remove genes not expressed in any sample
keep <- rowSums(raw_counts) > 0
counts <- raw_counts[keep,]

#log2 of raw read counts
logcounts <- log2(counts+1)

#select data for top 100 genes
select <-  order(rowMeans(counts), decreasing=T)[1:100]
highexprgenes_logcounts <- logcounts[select,]

```

Principal component analysis

```{r PCA}
#transpose matrix
data_for_PCA <- t(highexprgenes_logcounts)

#centre data, no scaling
bpc = prcomp(data_for_PCA, center=T, scale=F)

library(ggfortify)

#PCA plot with samples coloured by tissue
autoplot(bpc, main="PCA plot", data = info, colour='Tissue')

#which tissues the first few principal components are separating (or just look at PCA)

beigenvalues = bpc$sdev^2 # eigenvalues
beigenvectors = bpc$rotation # eigenvectors

plot(beigenvalues/sum(beigenvalues) * 100,xlab="PC",ylab="% Variance explained") 
plot(cumsum(beigenvalues)/sum(beigenvalues) * 100, xlab="PC",ylab="Cumulative % Variance explained")

#top genes correlated to PC1/PC2
library(pcaExplorer)
library(ggplot2)

hi_loadings(bpc, whichpc = 1, topN = 10, exprTable = NULL,
            annotation = NULL, title = "Top/bottom loadings - ")

hi_loadings(bpc, whichpc = 2, topN = 10, exprTable = NULL,
            annotation = NULL, title = "Top/bottom loadings - ")


#X-axis labels cut off so can't see full gene name - change to data frame so can use autocomplete to find gene name!
df <- data.frame(data_for_PCA)
df$ENSG00000131095.7 #top gene for PC1
df$ENSG00000120885.15 #top gene for PC2

#expression values per tissue
s = data_for_PCA[,which(colnames(data_for_PCA)=="ENSG00000131095.7")]
s2 = cbind(s,info)
ggplot(s2, aes(Tissue,s))+ geom_boxplot(position="dodge") +geom_point(alpha=0.6, aes(group=Tissue), data=s2, position = position_dodge(width=0.75))+ylab("log Expression")+xlab("Tissue")+ggtitle("Gene GFAP") + theme(axis.text.x  = element_text(angle=90, size=8))

s = data_for_PCA[,which(colnames(data_for_PCA)=="ENSG00000120885.15")]
s2 = cbind(s,info)
ggplot(s2, aes(Tissue,s))+ geom_boxplot(position="dodge") +geom_point(alpha=0.6, aes(group=Tissue), data=s2, position = position_dodge(width=0.75))+ylab("log Expression")+xlab("Tissue")+ggtitle("Gene CLU") + theme(axis.text.x  = element_text(angle=90, size=8))

```

t-SNE
```{r tSNE}
library("Rtsne")

iris_matrix <- as.matrix(data_for_PCA)
set.seed(42) # Set a seed if you want reproducible results
tsne_out <- Rtsne(iris_matrix, dims = 2, perplexity = 10) # Run TSNE. Alter perplexity to change clustering
# Show the objects in the 2D tsne representation
#plot(tsne_out$Y,col=info$Tissue)

dat = tsne_out$Y
colnames(dat) = c("tSNE1","tSNE2")
ggplot(dat, aes(tSNE1,tSNE2, color=info$Tissue))+geom_point() +ggtitle("tSNE") 

```

